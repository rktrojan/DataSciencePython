{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USING SKLEARN NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "\n",
    "\n",
    "reg = MLPRegressor(hidden_layer_sizes=(2,3),verbose=2,activation=\"relu\" ,batch_size=40,random_state=1, max_iter=2000, learning_rate_init=0.03)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "??MLPRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ON RANDOM DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [[22,33],\n",
    "           [44,55]]\n",
    "\n",
    "y_train = [22,34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 439.91277618\n",
      "Iteration 2, loss = 438.17080075\n",
      "Iteration 3, loss = 436.48627815\n",
      "Iteration 4, loss = 434.85949119\n",
      "Iteration 5, loss = 433.29007947\n",
      "Iteration 6, loss = 431.77675946\n",
      "Iteration 7, loss = 430.31693847\n",
      "Iteration 8, loss = 428.90622148\n",
      "Iteration 9, loss = 427.53787313\n",
      "Iteration 10, loss = 426.20241648\n",
      "Iteration 11, loss = 424.88768064\n",
      "Iteration 12, loss = 423.57958647\n",
      "Iteration 13, loss = 422.26361699\n",
      "Iteration 14, loss = 420.92642523\n",
      "Iteration 15, loss = 419.52103039\n",
      "Iteration 16, loss = 417.64031843\n",
      "Iteration 17, loss = 414.91432690\n",
      "Iteration 18, loss = 411.11946441\n",
      "Iteration 19, loss = 405.83735805\n",
      "Iteration 20, loss = 398.59220824\n",
      "Iteration 21, loss = 388.88394456\n",
      "Iteration 22, loss = 376.21225785\n",
      "Iteration 23, loss = 360.09838715\n",
      "Iteration 24, loss = 340.10889393\n",
      "Iteration 25, loss = 315.89219893\n",
      "Iteration 26, loss = 287.23422082\n",
      "Iteration 27, loss = 254.13606462\n",
      "Iteration 28, loss = 216.91697632\n",
      "Iteration 29, loss = 176.34701980\n",
      "Iteration 30, loss = 133.81386686\n",
      "Iteration 31, loss = 91.52332767\n",
      "Iteration 32, loss = 52.71323587\n",
      "Iteration 33, loss = 21.79072345\n",
      "Iteration 34, loss = 4.08172147\n",
      "Iteration 35, loss = 4.30281533\n",
      "Iteration 36, loss = 22.25782366\n",
      "Iteration 37, loss = 47.65669113\n",
      "Iteration 38, loss = 64.42023889\n",
      "Iteration 39, loss = 64.95149237\n",
      "Iteration 40, loss = 52.13195587\n",
      "Iteration 41, loss = 33.37578134\n",
      "Iteration 42, loss = 15.90360071\n",
      "Iteration 43, loss = 4.64800024\n",
      "Iteration 44, loss = 1.65401611\n",
      "Iteration 45, loss = 5.92982878\n",
      "Iteration 46, loss = 13.22131941\n",
      "Iteration 47, loss = 18.51365095\n",
      "Iteration 48, loss = 19.64237678\n",
      "Iteration 49, loss = 16.95494363\n",
      "Iteration 50, loss = 11.95418837\n",
      "Iteration 51, loss = 6.55702785\n",
      "Iteration 52, loss = 2.67352482\n",
      "Iteration 53, loss = 1.80496353\n",
      "Iteration 54, loss = 4.07224962\n",
      "Iteration 55, loss = 6.73959792\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:353: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped\n",
      "  warnings.warn(\"Got `batch_size` less than 1 or larger than \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(batch_size=40, hidden_layer_sizes=(2, 3), learning_rate_init=0.03,\n",
       "             max_iter=2000, random_state=1, verbose=2)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test =[[22,33],\n",
    "           [44,55]]\n",
    "\n",
    "y_test= [22,33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21.97003844, 39.37681381])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "y_pred=reg.predict(X_test)\n",
    "y_pred\n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7315827599624313"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ON DIABETES DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([151.,  75., 141., 206., 135.,  97., 138.,  63., 110., 310., 101.,\n",
       "        69., 179., 185., 118., 171., 166., 144.,  97., 168.,  68.,  49.,\n",
       "        68., 245., 184., 202., 137.,  85., 131., 283., 129.,  59., 341.,\n",
       "        87.,  65., 102., 265., 276., 252.,  90., 100.,  55.,  61.,  92.,\n",
       "       259.,  53., 190., 142.,  75., 142., 155., 225.,  59., 104., 182.,\n",
       "       128.,  52.,  37., 170., 170.,  61., 144.,  52., 128.,  71., 163.,\n",
       "       150.,  97., 160., 178.,  48., 270., 202., 111.,  85.,  42., 170.,\n",
       "       200., 252., 113., 143.,  51.,  52., 210.,  65., 141.,  55., 134.,\n",
       "        42., 111.,  98., 164.,  48.,  96.,  90., 162., 150., 279.,  92.,\n",
       "        83., 128., 102., 302., 198.,  95.,  53., 134., 144., 232.,  81.,\n",
       "       104.,  59., 246., 297., 258., 229., 275., 281., 179., 200., 200.,\n",
       "       173., 180.,  84., 121., 161.,  99., 109., 115., 268., 274., 158.,\n",
       "       107.,  83., 103., 272.,  85., 280., 336., 281., 118., 317., 235.,\n",
       "        60., 174., 259., 178., 128.,  96., 126., 288.,  88., 292.,  71.,\n",
       "       197., 186.,  25.,  84.,  96., 195.,  53., 217., 172., 131., 214.,\n",
       "        59.,  70., 220., 268., 152.,  47.,  74., 295., 101., 151., 127.,\n",
       "       237., 225.,  81., 151., 107.,  64., 138., 185., 265., 101., 137.,\n",
       "       143., 141.,  79., 292., 178.,  91., 116.,  86., 122.,  72., 129.,\n",
       "       142.,  90., 158.,  39., 196., 222., 277.,  99., 196., 202., 155.,\n",
       "        77., 191.,  70.,  73.,  49.,  65., 263., 248., 296., 214., 185.,\n",
       "        78.,  93., 252., 150.,  77., 208.,  77., 108., 160.,  53., 220.,\n",
       "       154., 259.,  90., 246., 124.,  67.,  72., 257., 262., 275., 177.,\n",
       "        71.,  47., 187., 125.,  78.,  51., 258., 215., 303., 243.,  91.,\n",
       "       150., 310., 153., 346.,  63.,  89.,  50.,  39., 103., 308., 116.,\n",
       "       145.,  74.,  45., 115., 264.,  87., 202., 127., 182., 241.,  66.,\n",
       "        94., 283.,  64., 102., 200., 265.,  94., 230., 181., 156., 233.,\n",
       "        60., 219.,  80.,  68., 332., 248.,  84., 200.,  55.,  85.,  89.,\n",
       "        31., 129.,  83., 275.,  65., 198., 236., 253., 124.,  44., 172.,\n",
       "       114., 142., 109., 180., 144., 163., 147.,  97., 220., 190., 109.,\n",
       "       191., 122., 230., 242., 248., 249., 192., 131., 237.,  78., 135.,\n",
       "       244., 199., 270., 164.,  72.,  96., 306.,  91., 214.,  95., 216.,\n",
       "       263., 178., 113., 200., 139., 139.,  88., 148.,  88., 243.,  71.,\n",
       "        77., 109., 272.,  60.,  54., 221.,  90., 311., 281., 182., 321.,\n",
       "        58., 262., 206., 233., 242., 123., 167.,  63., 197.,  71., 168.,\n",
       "       140., 217., 121., 235., 245.,  40.,  52., 104., 132.,  88.,  69.,\n",
       "       219.,  72., 201., 110.,  51., 277.,  63., 118.,  69., 273., 258.,\n",
       "        43., 198., 242., 232., 175.,  93., 168., 275., 293., 281.,  72.,\n",
       "       140., 189., 181., 209., 136., 261., 113., 131., 174., 257.,  55.,\n",
       "        84.,  42., 146., 212., 233.,  91., 111., 152., 120.,  67., 310.,\n",
       "        94., 183.,  66., 173.,  72.,  49.,  64.,  48., 178., 104., 132.,\n",
       "       220.,  57.])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "dataset = load_diabetes()\n",
    "dataset.data\n",
    "dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,\n",
       "          0.01990842, -0.01764613],\n",
       "        [-0.00188202, -0.04464164, -0.05147406, ..., -0.03949338,\n",
       "         -0.06832974, -0.09220405],\n",
       "        [ 0.08529891,  0.05068012,  0.04445121, ..., -0.00259226,\n",
       "          0.00286377, -0.02593034],\n",
       "        ...,\n",
       "        [ 0.04170844,  0.05068012, -0.01590626, ..., -0.01107952,\n",
       "         -0.04687948,  0.01549073],\n",
       "        [-0.04547248, -0.04464164,  0.03906215, ...,  0.02655962,\n",
       "          0.04452837, -0.02593034],\n",
       "        [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,\n",
       "         -0.00421986,  0.00306441]]),\n",
       " 'target': array([151.,  75., 141., 206., 135.,  97., 138.,  63., 110., 310., 101.,\n",
       "         69., 179., 185., 118., 171., 166., 144.,  97., 168.,  68.,  49.,\n",
       "         68., 245., 184., 202., 137.,  85., 131., 283., 129.,  59., 341.,\n",
       "         87.,  65., 102., 265., 276., 252.,  90., 100.,  55.,  61.,  92.,\n",
       "        259.,  53., 190., 142.,  75., 142., 155., 225.,  59., 104., 182.,\n",
       "        128.,  52.,  37., 170., 170.,  61., 144.,  52., 128.,  71., 163.,\n",
       "        150.,  97., 160., 178.,  48., 270., 202., 111.,  85.,  42., 170.,\n",
       "        200., 252., 113., 143.,  51.,  52., 210.,  65., 141.,  55., 134.,\n",
       "         42., 111.,  98., 164.,  48.,  96.,  90., 162., 150., 279.,  92.,\n",
       "         83., 128., 102., 302., 198.,  95.,  53., 134., 144., 232.,  81.,\n",
       "        104.,  59., 246., 297., 258., 229., 275., 281., 179., 200., 200.,\n",
       "        173., 180.,  84., 121., 161.,  99., 109., 115., 268., 274., 158.,\n",
       "        107.,  83., 103., 272.,  85., 280., 336., 281., 118., 317., 235.,\n",
       "         60., 174., 259., 178., 128.,  96., 126., 288.,  88., 292.,  71.,\n",
       "        197., 186.,  25.,  84.,  96., 195.,  53., 217., 172., 131., 214.,\n",
       "         59.,  70., 220., 268., 152.,  47.,  74., 295., 101., 151., 127.,\n",
       "        237., 225.,  81., 151., 107.,  64., 138., 185., 265., 101., 137.,\n",
       "        143., 141.,  79., 292., 178.,  91., 116.,  86., 122.,  72., 129.,\n",
       "        142.,  90., 158.,  39., 196., 222., 277.,  99., 196., 202., 155.,\n",
       "         77., 191.,  70.,  73.,  49.,  65., 263., 248., 296., 214., 185.,\n",
       "         78.,  93., 252., 150.,  77., 208.,  77., 108., 160.,  53., 220.,\n",
       "        154., 259.,  90., 246., 124.,  67.,  72., 257., 262., 275., 177.,\n",
       "         71.,  47., 187., 125.,  78.,  51., 258., 215., 303., 243.,  91.,\n",
       "        150., 310., 153., 346.,  63.,  89.,  50.,  39., 103., 308., 116.,\n",
       "        145.,  74.,  45., 115., 264.,  87., 202., 127., 182., 241.,  66.,\n",
       "         94., 283.,  64., 102., 200., 265.,  94., 230., 181., 156., 233.,\n",
       "         60., 219.,  80.,  68., 332., 248.,  84., 200.,  55.,  85.,  89.,\n",
       "         31., 129.,  83., 275.,  65., 198., 236., 253., 124.,  44., 172.,\n",
       "        114., 142., 109., 180., 144., 163., 147.,  97., 220., 190., 109.,\n",
       "        191., 122., 230., 242., 248., 249., 192., 131., 237.,  78., 135.,\n",
       "        244., 199., 270., 164.,  72.,  96., 306.,  91., 214.,  95., 216.,\n",
       "        263., 178., 113., 200., 139., 139.,  88., 148.,  88., 243.,  71.,\n",
       "         77., 109., 272.,  60.,  54., 221.,  90., 311., 281., 182., 321.,\n",
       "         58., 262., 206., 233., 242., 123., 167.,  63., 197.,  71., 168.,\n",
       "        140., 217., 121., 235., 245.,  40.,  52., 104., 132.,  88.,  69.,\n",
       "        219.,  72., 201., 110.,  51., 277.,  63., 118.,  69., 273., 258.,\n",
       "         43., 198., 242., 232., 175.,  93., 168., 275., 293., 281.,  72.,\n",
       "        140., 189., 181., 209., 136., 261., 113., 131., 174., 257.,  55.,\n",
       "         84.,  42., 146., 212., 233.,  91., 111., 152., 120.,  67., 310.,\n",
       "         94., 183.,  66., 173.,  72.,  49.,  64.,  48., 178., 104., 132.,\n",
       "        220.,  57.]),\n",
       " 'frame': None,\n",
       " 'DESCR': '.. _diabetes_dataset:\\n\\nDiabetes dataset\\n----------------\\n\\nTen baseline variables, age, sex, body mass index, average blood\\npressure, and six blood serum measurements were obtained for each of n =\\n442 diabetes patients, as well as the response of interest, a\\nquantitative measure of disease progression one year after baseline.\\n\\n**Data Set Characteristics:**\\n\\n  :Number of Instances: 442\\n\\n  :Number of Attributes: First 10 columns are numeric predictive values\\n\\n  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\\n\\n  :Attribute Information:\\n      - age     age in years\\n      - sex\\n      - bmi     body mass index\\n      - bp      average blood pressure\\n      - s1      tc, T-Cells (a type of white blood cells)\\n      - s2      ldl, low-density lipoproteins\\n      - s3      hdl, high-density lipoproteins\\n      - s4      tch, thyroid stimulating hormone\\n      - s5      ltg, lamotrigine\\n      - s6      glu, blood sugar level\\n\\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).\\n\\nSource URL:\\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\\n\\nFor more information see:\\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)',\n",
       " 'feature_names': ['age',\n",
       "  'sex',\n",
       "  'bmi',\n",
       "  'bp',\n",
       "  's1',\n",
       "  's2',\n",
       "  's3',\n",
       "  's4',\n",
       "  's5',\n",
       "  's6'],\n",
       " 'data_filename': 'C:\\\\Users\\\\Admin\\\\AppData\\\\Roaming\\\\Python\\\\Python37\\\\site-packages\\\\sklearn\\\\datasets\\\\data\\\\diabetes_data.csv.gz',\n",
       " 'target_filename': 'C:\\\\Users\\\\Admin\\\\AppData\\\\Roaming\\\\Python\\\\Python37\\\\site-packages\\\\sklearn\\\\datasets\\\\data\\\\diabetes_target.csv.gz'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442, 10)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 14261.73592157\n",
      "Iteration 2, loss = 13841.69083599\n",
      "Iteration 3, loss = 13207.64840426\n",
      "Iteration 4, loss = 12210.66399938\n",
      "Iteration 5, loss = 10757.22194762\n",
      "Iteration 6, loss = 8798.48559890\n",
      "Iteration 7, loss = 6452.41012238\n",
      "Iteration 8, loss = 4287.18452799\n",
      "Iteration 9, loss = 2791.67751079\n",
      "Iteration 10, loss = 2226.85054686\n",
      "Iteration 11, loss = 2125.21506172\n",
      "Iteration 12, loss = 2072.02224204\n",
      "Iteration 13, loss = 2009.84855150\n",
      "Iteration 14, loss = 1957.03336715\n",
      "Iteration 15, loss = 1905.02473547\n",
      "Iteration 16, loss = 1883.24170264\n",
      "Iteration 17, loss = 1845.68675786\n",
      "Iteration 18, loss = 1812.23290522\n",
      "Iteration 19, loss = 1790.32313530\n",
      "Iteration 20, loss = 1763.17124737\n",
      "Iteration 21, loss = 1738.47589115\n",
      "Iteration 22, loss = 1711.74614917\n",
      "Iteration 23, loss = 1691.86953602\n",
      "Iteration 24, loss = 1670.70861356\n",
      "Iteration 25, loss = 1655.62283319\n",
      "Iteration 26, loss = 1642.23655975\n",
      "Iteration 27, loss = 1628.36243822\n",
      "Iteration 28, loss = 1623.42158308\n",
      "Iteration 29, loss = 1605.62297465\n",
      "Iteration 30, loss = 1594.48956679\n",
      "Iteration 31, loss = 1589.90053067\n",
      "Iteration 32, loss = 1575.11136053\n",
      "Iteration 33, loss = 1586.89999928\n",
      "Iteration 34, loss = 1568.36818389\n",
      "Iteration 35, loss = 1561.39833364\n",
      "Iteration 36, loss = 1554.60140553\n",
      "Iteration 37, loss = 1546.89470538\n",
      "Iteration 38, loss = 1530.33521712\n",
      "Iteration 39, loss = 1538.61490008\n",
      "Iteration 40, loss = 1541.98348094\n",
      "Iteration 41, loss = 1514.66895113\n",
      "Iteration 42, loss = 1510.82231169\n",
      "Iteration 43, loss = 1512.37057433\n",
      "Iteration 44, loss = 1493.99119287\n",
      "Iteration 45, loss = 1506.98709589\n",
      "Iteration 46, loss = 1524.57615541\n",
      "Iteration 47, loss = 1492.65194992\n",
      "Iteration 48, loss = 1491.10463887\n",
      "Iteration 49, loss = 1481.53334686\n",
      "Iteration 50, loss = 1486.12748116\n",
      "Iteration 51, loss = 1480.58548580\n",
      "Iteration 52, loss = 1479.81685214\n",
      "Iteration 53, loss = 1467.47906409\n",
      "Iteration 54, loss = 1468.92292709\n",
      "Iteration 55, loss = 1466.99639809\n",
      "Iteration 56, loss = 1464.90500830\n",
      "Iteration 57, loss = 1476.14563957\n",
      "Iteration 58, loss = 1468.50971111\n",
      "Iteration 59, loss = 1458.80259865\n",
      "Iteration 60, loss = 1474.81019088\n",
      "Iteration 61, loss = 1468.91349562\n",
      "Iteration 62, loss = 1460.53336113\n",
      "Iteration 63, loss = 1465.51774192\n",
      "Iteration 64, loss = 1462.67384480\n",
      "Iteration 65, loss = 1458.53364859\n",
      "Iteration 66, loss = 1457.22907707\n",
      "Iteration 67, loss = 1457.78820936\n",
      "Iteration 68, loss = 1464.98864100\n",
      "Iteration 69, loss = 1470.16507471\n",
      "Iteration 70, loss = 1460.84445199\n",
      "Iteration 71, loss = 1460.41951067\n",
      "Iteration 72, loss = 1459.41255123\n",
      "Iteration 73, loss = 1451.96865929\n",
      "Iteration 74, loss = 1452.30556545\n",
      "Iteration 75, loss = 1450.62640215\n",
      "Iteration 76, loss = 1451.87352289\n",
      "Iteration 77, loss = 1452.79931115\n",
      "Iteration 78, loss = 1455.41620322\n",
      "Iteration 79, loss = 1451.45464694\n",
      "Iteration 80, loss = 1451.69727391\n",
      "Iteration 81, loss = 1451.90983822\n",
      "Iteration 82, loss = 1450.15947967\n",
      "Iteration 83, loss = 1451.89277496\n",
      "Iteration 84, loss = 1453.83043956\n",
      "Iteration 85, loss = 1448.91911361\n",
      "Iteration 86, loss = 1457.62841167\n",
      "Iteration 87, loss = 1458.98361637\n",
      "Iteration 88, loss = 1450.02365108\n",
      "Iteration 89, loss = 1447.22725036\n",
      "Iteration 90, loss = 1466.28037873\n",
      "Iteration 91, loss = 1469.79510431\n",
      "Iteration 92, loss = 1470.84521466\n",
      "Iteration 93, loss = 1456.94035461\n",
      "Iteration 94, loss = 1446.24855576\n",
      "Iteration 95, loss = 1448.59083257\n",
      "Iteration 96, loss = 1446.17150104\n",
      "Iteration 97, loss = 1446.19305814\n",
      "Iteration 98, loss = 1451.03820749\n",
      "Iteration 99, loss = 1443.21982244\n",
      "Iteration 100, loss = 1466.25069746\n",
      "Iteration 101, loss = 1450.69436569\n",
      "Iteration 102, loss = 1445.42878520\n",
      "Iteration 103, loss = 1446.89004020\n",
      "Iteration 104, loss = 1447.60119830\n",
      "Iteration 105, loss = 1447.88991114\n",
      "Iteration 106, loss = 1446.08141478\n",
      "Iteration 107, loss = 1449.21081402\n",
      "Iteration 108, loss = 1451.23655171\n",
      "Iteration 109, loss = 1445.92669540\n",
      "Iteration 110, loss = 1453.82910236\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(batch_size=40, hidden_layer_sizes=(2, 3), learning_rate_init=0.03,\n",
       "             max_iter=2000, random_state=1, verbose=2)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "reg.fit(dataset.data,dataset.target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.1162057 , -0.13600488],\n",
       "        [-7.37855596, -0.1178    ],\n",
       "        [14.07295837, -0.96833123],\n",
       "        [ 9.47925236, -0.84401587],\n",
       "        [-1.14516051,  0.07033046],\n",
       "        [-3.69113787,  0.38876881],\n",
       "        [-6.90107354,  0.63306544],\n",
       "        [ 3.2959029 ,  0.09042091],\n",
       "        [12.38212846, -0.41874271],\n",
       "        [ 2.81301137, -0.67490674]]),\n",
       " array([[-4.32990874e-01,  5.74726960e+00,  7.06264621e-32],\n",
       "        [-1.58035453e-30, -8.65924783e-02,  1.60186408e-30]]),\n",
       " array([[ 5.08589288e-01],\n",
       "        [ 6.06367546e+00],\n",
       "        [-6.20468760e-06]])]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coefs_\n",
    "\n",
    "\n",
    "#weigth matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#??reg.coefs_\n",
    "\n",
    "X_test= [[ 0.03807591,.03807591,  0.05068012,  0.06169621,0.3807591 , -0.00259226,\n",
    "          0.01990842, -0.01764613, 0.3807591, 0.3807591],\n",
    "         \n",
    "         [ 0.03807591,.03807591,  0.05068012,  0.06169621,0.3807591 , -0.00259226,\n",
    "          0.01990842, -0.01764613, 0.3807591, 0.4444]\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = [300,300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([363.01912703, 369.25798817])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y_pred=reg.predict(X_test)\n",
    "y_pred\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-449.5301145513"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  \n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USING KERAS MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dependencies\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ON DIABETES DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "dataset = load_diabetes()\n",
    "dataset.data\n",
    "dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\ProgramData\\\\Anaconda3',\n",
       " 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import site\n",
    "site.getsitepackages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: (10,)\n"
     ]
    }
   ],
   "source": [
    "# Set the input shape\n",
    "input_shape = (10,)\n",
    "print(f'Feature shape: {input_shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "\n",
    "model.add(Dense(16, input_shape=input_shape, activation='relu'))\n",
    "\n",
    "model.add(Dense(8, activation='relu'))\n",
    "\n",
    "model.add(Dense(1, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442, 10)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b =1,2\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 95/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 48ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f90f84c748>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure the model \n",
    "\n",
    "dataset\n",
    "\n",
    "x_train, y_train = dataset.data, dataset.target\n",
    "x_test, y_test\n",
    "\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "\n",
    "#and start training\n",
    "model.fit(x_train, y_train, epochs=100, batch_size=442, verbose=1, validation_split=0.2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 16)                176       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 321\n",
      "Trainable params: 321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ON CHENNAI WATER PROBLEM DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3.9,   0. , 268. ,   0. ],\n",
       "       [  3.9,   0. , 268. ,   0. ],\n",
       "       [  3.9,   0. , 267. ,   0. ],\n",
       "       ...,\n",
       "       [ 27. ,   0. ,   0. ,   1. ],\n",
       "       [ 26. ,   0. ,   0. ,   1. ],\n",
       "       [ 25. ,   0. ,   0. ,   1. ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "dataset = np.loadtxt('chennai_reservoir_levels.csv', delimiter='|', skiprows=1, usecols=(1,2,3,4))\n",
    "dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Shuffle dataset\n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "# Separate features and targets\n",
    "X = dataset[:, 0:3]\n",
    "Y = dataset[:, 3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#follow the keras process as done for diabetes dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
