{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USING SKLEARN NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "reg = MLPClassifier(hidden_layer_sizes=(2,3),\n",
    "                    verbose=2,\n",
    "                    activation=\"relu\" ,\n",
    "                    batch_size=40,\n",
    "                    random_state=1, \n",
    "                    max_iter=2000, \n",
    "                    learning_rate_init=0.03)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "??MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ON RANDOM DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [[22,33],\n",
    "           [44,55]]\n",
    "\n",
    "y_train = [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.82481770\n",
      "Iteration 2, loss = 0.81072171\n",
      "Iteration 3, loss = 0.79776556\n",
      "Iteration 4, loss = 0.78591496\n",
      "Iteration 5, loss = 0.77512384\n",
      "Iteration 6, loss = 0.76533503\n",
      "Iteration 7, loss = 0.75648167\n",
      "Iteration 8, loss = 0.74848957\n",
      "Iteration 9, loss = 0.74128057\n",
      "Iteration 10, loss = 0.73477665\n",
      "Iteration 11, loss = 0.72890442\n",
      "Iteration 12, loss = 0.72359893\n",
      "Iteration 13, loss = 0.71880638\n",
      "Iteration 14, loss = 0.71448509\n",
      "Iteration 15, loss = 0.71005126\n",
      "Iteration 16, loss = 0.69956078\n",
      "Iteration 17, loss = 0.68669849\n",
      "Iteration 18, loss = 0.66973401\n",
      "Iteration 19, loss = 0.64899605\n",
      "Iteration 20, loss = 0.62857226\n",
      "Iteration 21, loss = 0.61767667\n",
      "Iteration 22, loss = 0.62753616\n",
      "Iteration 23, loss = 0.64651495\n",
      "Iteration 24, loss = 0.64697809\n",
      "Iteration 25, loss = 0.63232672\n",
      "Iteration 26, loss = 0.61643039\n",
      "Iteration 27, loss = 0.60760683\n",
      "Iteration 28, loss = 0.60614063\n",
      "Iteration 29, loss = 0.60802059\n",
      "Iteration 30, loss = 0.60942583\n",
      "Iteration 31, loss = 0.60823544\n",
      "Iteration 32, loss = 0.60367383\n",
      "Iteration 33, loss = 0.59588074\n",
      "Iteration 34, loss = 0.58585924\n",
      "Iteration 35, loss = 0.57553890\n",
      "Iteration 36, loss = 0.56736003\n",
      "Iteration 37, loss = 0.56274436\n",
      "Iteration 38, loss = 0.56006909\n",
      "Iteration 39, loss = 0.55523115\n",
      "Iteration 40, loss = 0.54586124\n",
      "Iteration 41, loss = 0.53378189\n",
      "Iteration 42, loss = 0.52266527\n",
      "Iteration 43, loss = 0.51436458\n",
      "Iteration 44, loss = 0.50742877\n",
      "Iteration 45, loss = 0.49850499\n",
      "Iteration 46, loss = 0.48532673\n",
      "Iteration 47, loss = 0.46895676\n",
      "Iteration 48, loss = 0.45290727\n",
      "Iteration 49, loss = 0.43848650\n",
      "Iteration 50, loss = 0.42182060\n",
      "Iteration 51, loss = 0.40009189\n",
      "Iteration 52, loss = 0.37756797\n",
      "Iteration 53, loss = 0.35810815\n",
      "Iteration 54, loss = 0.33412530\n",
      "Iteration 55, loss = 0.30517067\n",
      "Iteration 56, loss = 0.28073992\n",
      "Iteration 57, loss = 0.25327360\n",
      "Iteration 58, loss = 0.24557065\n",
      "Iteration 59, loss = 0.21365956\n",
      "Iteration 60, loss = 0.19366877\n",
      "Iteration 61, loss = 0.20179384\n",
      "Iteration 62, loss = 0.18935388\n",
      "Iteration 63, loss = 0.16738017\n",
      "Iteration 64, loss = 0.18009649\n",
      "Iteration 65, loss = 0.15271117\n",
      "Iteration 66, loss = 0.15127405\n",
      "Iteration 67, loss = 0.14849656\n",
      "Iteration 68, loss = 0.14417533\n",
      "Iteration 69, loss = 0.13872283\n",
      "Iteration 70, loss = 0.13340166\n",
      "Iteration 71, loss = 0.13287417\n",
      "Iteration 72, loss = 0.12923599\n",
      "Iteration 73, loss = 0.13536980\n",
      "Iteration 74, loss = 0.14521452\n",
      "Iteration 75, loss = 0.14026298\n",
      "Iteration 76, loss = 0.12573179\n",
      "Iteration 77, loss = 0.11639298\n",
      "Iteration 78, loss = 0.11154653\n",
      "Iteration 79, loss = 0.10844978\n",
      "Iteration 80, loss = 0.10594372\n",
      "Iteration 81, loss = 0.15442436\n",
      "Iteration 82, loss = 0.10159136\n",
      "Iteration 83, loss = 0.10004861\n",
      "Iteration 84, loss = 0.09997831\n",
      "Iteration 85, loss = 0.10316968\n",
      "Iteration 86, loss = 0.10909066\n",
      "Iteration 87, loss = 0.10869712\n",
      "Iteration 88, loss = 0.09992467\n",
      "Iteration 89, loss = 0.09218552\n",
      "Iteration 90, loss = 0.08770965\n",
      "Iteration 91, loss = 0.08498666\n",
      "Iteration 92, loss = 0.08297631\n",
      "Iteration 93, loss = 0.08125452\n",
      "Iteration 94, loss = 0.07966605\n",
      "Iteration 95, loss = 0.07815378\n",
      "Iteration 96, loss = 0.09238166\n",
      "Iteration 97, loss = 0.07530337\n",
      "Iteration 98, loss = 0.07402772\n",
      "Iteration 99, loss = 0.07298796\n",
      "Iteration 100, loss = 0.07242553\n",
      "Iteration 101, loss = 0.07268556\n",
      "Iteration 102, loss = 0.07383869\n",
      "Iteration 103, loss = 0.07480491\n",
      "Iteration 104, loss = 0.07372283\n",
      "Iteration 105, loss = 0.07066118\n",
      "Iteration 106, loss = 0.06740084\n",
      "Iteration 107, loss = 0.06487873\n",
      "Iteration 108, loss = 0.06304644\n",
      "Iteration 109, loss = 0.06163726\n",
      "Iteration 110, loss = 0.06045917\n",
      "Iteration 111, loss = 0.05940779\n",
      "Iteration 112, loss = 0.05843090\n",
      "Iteration 113, loss = 0.05750265\n",
      "Iteration 114, loss = 0.05660994\n",
      "Iteration 115, loss = 0.05574584\n",
      "Iteration 116, loss = 0.05490648\n",
      "Iteration 117, loss = 0.05408949\n",
      "Iteration 118, loss = 0.05329336\n",
      "Iteration 119, loss = 0.05251696\n",
      "Iteration 120, loss = 0.05711276\n",
      "Iteration 121, loss = 0.05102957\n",
      "Iteration 122, loss = 0.05034862\n",
      "Iteration 123, loss = 0.04976481\n",
      "Iteration 124, loss = 0.04937821\n",
      "Iteration 125, loss = 0.04935468\n",
      "Iteration 126, loss = 0.04985616\n",
      "Iteration 127, loss = 0.05074996\n",
      "Iteration 128, loss = 0.05125863\n",
      "Iteration 129, loss = 0.05050582\n",
      "Iteration 130, loss = 0.04871972\n",
      "Iteration 131, loss = 0.04682248\n",
      "Iteration 132, loss = 0.04530922\n",
      "Iteration 133, loss = 0.04419062\n",
      "Iteration 134, loss = 0.04333669\n",
      "Iteration 135, loss = 0.04263770\n",
      "Iteration 136, loss = 0.04202699\n",
      "Iteration 137, loss = 0.04146808\n",
      "Iteration 138, loss = 0.04094161\n",
      "Iteration 139, loss = 0.04043722\n",
      "Iteration 140, loss = 0.03994923\n",
      "Iteration 141, loss = 0.03947446\n",
      "Iteration 142, loss = 0.03901100\n",
      "Iteration 143, loss = 0.03855769\n",
      "Iteration 144, loss = 0.03811375\n",
      "Iteration 145, loss = 0.03767864\n",
      "Iteration 146, loss = 0.03725197\n",
      "Iteration 147, loss = 0.03683343\n",
      "Iteration 148, loss = 0.03642274\n",
      "Iteration 149, loss = 0.03601967\n",
      "Iteration 150, loss = 0.03601774\n",
      "Iteration 151, loss = 0.03524241\n",
      "Iteration 152, loss = 0.03487833\n",
      "Iteration 153, loss = 0.03454334\n",
      "Iteration 154, loss = 0.03425746\n",
      "Iteration 155, loss = 0.03405053\n",
      "Iteration 156, loss = 0.03395765\n",
      "Iteration 157, loss = 0.03400028\n",
      "Iteration 158, loss = 0.03414749\n",
      "Iteration 159, loss = 0.03427829\n",
      "Iteration 160, loss = 0.03421139\n",
      "Iteration 161, loss = 0.03383427\n",
      "Iteration 162, loss = 0.03320250\n",
      "Iteration 163, loss = 0.03247521\n",
      "Iteration 164, loss = 0.03178482\n",
      "Iteration 165, loss = 0.03118657\n",
      "Iteration 166, loss = 0.03068152\n",
      "Iteration 167, loss = 0.03025031\n",
      "Iteration 168, loss = 0.02987197\n",
      "Iteration 169, loss = 0.02953006\n",
      "Iteration 170, loss = 0.02921322\n",
      "Iteration 171, loss = 0.02891397\n",
      "Iteration 172, loss = 0.02862752\n",
      "Iteration 173, loss = 0.02835077\n",
      "Iteration 174, loss = 0.02808172\n",
      "Iteration 175, loss = 0.02781907\n",
      "Iteration 176, loss = 0.02756194\n",
      "Iteration 177, loss = 0.02730971\n",
      "Iteration 178, loss = 0.02706196\n",
      "Iteration 179, loss = 0.02681839\n",
      "Iteration 180, loss = 0.02657876\n",
      "Iteration 181, loss = 0.02634288\n",
      "Iteration 182, loss = 0.02611063\n",
      "Iteration 183, loss = 0.02588187\n",
      "Iteration 184, loss = 0.02565651\n",
      "Iteration 185, loss = 0.02543446\n",
      "Iteration 186, loss = 0.02521563\n",
      "Iteration 187, loss = 0.02499995\n",
      "Iteration 188, loss = 0.02478736\n",
      "Iteration 189, loss = 0.02457779\n",
      "Iteration 190, loss = 0.02437118\n",
      "Iteration 191, loss = 0.02416747\n",
      "Iteration 192, loss = 0.02396660\n",
      "Iteration 193, loss = 0.02376853\n",
      "Iteration 194, loss = 0.02357318\n",
      "Iteration 195, loss = 0.02338053\n",
      "Iteration 196, loss = 0.02319051\n",
      "Iteration 197, loss = 0.02300307\n",
      "Iteration 198, loss = 0.02281817\n",
      "Iteration 199, loss = 0.02263576\n",
      "Iteration 200, loss = 0.02245580\n",
      "Iteration 201, loss = 0.02227824\n",
      "Iteration 202, loss = 0.02210303\n",
      "Iteration 203, loss = 0.02193014\n",
      "Iteration 204, loss = 0.02175952\n",
      "Iteration 205, loss = 0.02159112\n",
      "Iteration 206, loss = 0.02142492\n",
      "Iteration 207, loss = 0.02126087\n",
      "Iteration 208, loss = 0.02109893\n",
      "Iteration 209, loss = 0.02093906\n",
      "Iteration 210, loss = 0.02078123\n",
      "Iteration 211, loss = 0.02062541\n",
      "Iteration 212, loss = 0.02047154\n",
      "Iteration 213, loss = 0.02031961\n",
      "Iteration 214, loss = 0.02016958\n",
      "Iteration 215, loss = 0.02002141\n",
      "Iteration 216, loss = 0.01987507\n",
      "Iteration 217, loss = 0.01973053\n",
      "Iteration 218, loss = 0.01958775\n",
      "Iteration 219, loss = 0.01944672\n",
      "Iteration 220, loss = 0.01930740\n",
      "Iteration 221, loss = 0.01916975\n",
      "Iteration 222, loss = 0.01903375\n",
      "Iteration 223, loss = 0.01889938\n",
      "Iteration 224, loss = 0.01876660\n",
      "Iteration 225, loss = 0.01863539\n",
      "Iteration 226, loss = 0.01850573\n",
      "Iteration 227, loss = 0.01837758\n",
      "Iteration 228, loss = 0.01825092\n",
      "Iteration 229, loss = 0.01812573\n",
      "Iteration 230, loss = 0.01800198\n",
      "Iteration 231, loss = 0.01787965\n",
      "Iteration 232, loss = 0.01775872\n",
      "Iteration 233, loss = 0.01763916\n",
      "Iteration 234, loss = 0.01752095\n",
      "Iteration 235, loss = 0.01740407\n",
      "Iteration 236, loss = 0.01728851\n",
      "Iteration 237, loss = 0.01717423\n",
      "Iteration 238, loss = 0.01706122\n",
      "Iteration 239, loss = 0.01694945\n",
      "Iteration 240, loss = 0.01683892\n",
      "Iteration 241, loss = 0.01672959\n",
      "Iteration 242, loss = 0.01662146\n",
      "Iteration 243, loss = 0.01651449\n",
      "Iteration 244, loss = 0.01640868\n",
      "Iteration 245, loss = 0.01630401\n",
      "Iteration 246, loss = 0.01620046\n",
      "Iteration 247, loss = 0.01609801\n",
      "Iteration 248, loss = 0.01599664\n",
      "Iteration 249, loss = 0.01589635\n",
      "Iteration 250, loss = 0.01579710\n",
      "Iteration 251, loss = 0.01569890\n",
      "Iteration 252, loss = 0.01560172\n",
      "Iteration 253, loss = 0.01550554\n",
      "Iteration 254, loss = 0.01541036\n",
      "Iteration 255, loss = 0.01531615\n",
      "Iteration 256, loss = 0.01522291\n",
      "Iteration 257, loss = 0.01513062\n",
      "Iteration 258, loss = 0.01503926\n",
      "Iteration 259, loss = 0.01494883\n",
      "Iteration 260, loss = 0.01485931\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:611: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(batch_size=40, hidden_layer_sizes=(2, 3), learning_rate_init=0.03,\n",
       "              max_iter=2000, random_state=1, verbose=2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test =[[22,33],\n",
    "           [44,55]]\n",
    "\n",
    "y_test= [0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "y_pred=reg.predict(X_test)\n",
    "y_pred\n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.71121293e-01, 2.88787074e-02],\n",
       "       [9.97382736e-06, 9.99990026e-01]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y_pred_prob=reg.predict_proba(X_test)\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USING KERAS MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dependencies\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.initializers.initializers_v2.RandomNormal at 0x26b45f0a1c0>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import initializers\n",
    "\n",
    "\n",
    "initializers.RandomNormal(mean=0., stddev=1.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ON RANDOM DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test =[[22.,33.],\n",
    "           [44.,55.]]\n",
    "\n",
    "X_train =[[22.,33.],\n",
    "           [44.,55.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[22.0, 33.0], [44.0, 55.0]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=[0, 1]\n",
    "y_test=[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our vectorized labels\n",
    "\n",
    "y_train = np.asarray(y_train).astype('float32')#.reshape((-1,1))\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "y_test = np.asarray(y_test).astype('float32')#.reshape((-1,1))\n",
    "\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: (8,)\n"
     ]
    }
   ],
   "source": [
    "# Set the input shape\n",
    "\n",
    "input_shape = (8,)\n",
    "\n",
    "print(f'Feature shape: {input_shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "\n",
    "model.add(Dense(3, input_shape=input_shape, activation='relu',kernel_initializer=initializers.RandomNormal(mean=0., stddev=1.)))\n",
    "\n",
    "model.add(Dense(3, activation='relu',kernel_initializer=initializers.RandomNormal(mean=0., stddev=1.)))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid',kernel_initializer=initializers.RandomNormal(mean=0., stddev=1.)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "??model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If you're doing a binary classification, make sure that that your last Dense layer has just (None, 1) in its shape, not None, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the model \n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "\n",
    "#metrics=[tf.keras.metrics.CategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[22.0, 33.0], [44.0, 55.0]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "35/35 [==============================] - 1s 2ms/step - loss: 99.4038 - accuracy: 0.3444\n",
      "Epoch 2/100\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 77.8238 - accuracy: 0.3459\n",
      "Epoch 3/100\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 54.1090 - accuracy: 0.3473\n",
      "Epoch 4/100\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 26.7539 - accuracy: 0.3719\n",
      "Epoch 5/100\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 10.8938 - accuracy: 0.5065\n",
      "Epoch 6/100\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 9.3139 - accuracy: 0.5412\n",
      "Epoch 7/100\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 8.3075 - accuracy: 0.5586\n",
      "Epoch 8/100\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 7.5462 - accuracy: 0.5644\n",
      "Epoch 9/100\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 6.9932 - accuracy: 0.5977\n",
      "Epoch 10/100\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 6.5304 - accuracy: 0.5948\n",
      "Epoch 11/100\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 6.2272 - accuracy: 0.6093\n",
      "Epoch 12/100\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 5.9364 - accuracy: 0.6237\n",
      "Epoch 13/100\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 5.6753 - accuracy: 0.6151\n",
      "Epoch 14/100\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 5.5433 - accuracy: 0.6093\n",
      "Epoch 15/100\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 5.3266 - accuracy: 0.6093\n",
      "Epoch 16/100\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 5.1728 - accuracy: 0.6165\n",
      "Epoch 17/100\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 5.0448 - accuracy: 0.5991\n",
      "Epoch 18/100\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 4.9062 - accuracy: 0.6194\n",
      "Epoch 19/100\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 4.7535 - accuracy: 0.6165\n",
      "Epoch 20/100\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 4.6051 - accuracy: 0.6151\n",
      "Epoch 21/100\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 4.4731 - accuracy: 0.6151\n",
      "Epoch 22/100\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 4.3668 - accuracy: 0.6151\n",
      "Epoch 23/100\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 4.2366 - accuracy: 0.6107\n",
      "Epoch 24/100\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 4.1229 - accuracy: 0.6064\n",
      "Epoch 25/100\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 4.0265 - accuracy: 0.6107\n",
      "Epoch 26/100\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 3.8862 - accuracy: 0.6122\n",
      "Epoch 27/100\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 3.7727 - accuracy: 0.6093\n",
      "Epoch 28/100\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 3.6559 - accuracy: 0.6179\n",
      "Epoch 29/100\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 3.5919 - accuracy: 0.6035\n",
      "Epoch 30/100\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 3.4846 - accuracy: 0.6049\n",
      "Epoch 31/100\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 3.3526 - accuracy: 0.6093\n",
      "Epoch 32/100\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 3.2511 - accuracy: 0.6122\n",
      "Epoch 33/100\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 3.1591 - accuracy: 0.6064\n",
      "Epoch 34/100\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 3.0642 - accuracy: 0.6049\n",
      "Epoch 35/100\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 2.9909 - accuracy: 0.6122\n",
      "Epoch 36/100\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 2.9072 - accuracy: 0.6064\n",
      "Epoch 37/100\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 2.8405 - accuracy: 0.6078\n",
      "Epoch 38/100\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 2.7453 - accuracy: 0.6151\n",
      "Epoch 39/100\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 2.7408 - accuracy: 0.6049\n",
      "Epoch 40/100\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 2.5755 - accuracy: 0.6179\n",
      "Epoch 41/100\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 2.5101 - accuracy: 0.6194\n",
      "Epoch 42/100\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 2.4233 - accuracy: 0.6223\n",
      "Epoch 43/100\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 2.3907 - accuracy: 0.6078\n",
      "Epoch 44/100\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 2.3592 - accuracy: 0.6107\n",
      "Epoch 45/100\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 2.2560 - accuracy: 0.6136\n",
      "Epoch 46/100\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 2.1556 - accuracy: 0.6237\n",
      "Epoch 47/100\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 2.0843 - accuracy: 0.6194\n",
      "Epoch 48/100\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 2.0611 - accuracy: 0.6252\n",
      "Epoch 49/100\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.9896 - accuracy: 0.6179\n",
      "Epoch 50/100\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 1.9108 - accuracy: 0.6266\n",
      "Epoch 51/100\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.8440 - accuracy: 0.6122\n",
      "Epoch 52/100\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.8276 - accuracy: 0.6266\n",
      "Epoch 53/100\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 1.8004 - accuracy: 0.6165\n",
      "Epoch 54/100\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 1.7189 - accuracy: 0.6165\n",
      "Epoch 55/100\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 1.6500 - accuracy: 0.6179\n",
      "Epoch 56/100\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 1.6045 - accuracy: 0.6295\n",
      "Epoch 57/100\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 1.5458 - accuracy: 0.6223\n",
      "Epoch 58/100\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.5328 - accuracy: 0.6179\n",
      "Epoch 59/100\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 1.4666 - accuracy: 0.6208\n",
      "Epoch 60/100\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 1.4188 - accuracy: 0.6324\n",
      "Epoch 61/100\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 1.3943 - accuracy: 0.6208\n",
      "Epoch 62/100\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 1.3579 - accuracy: 0.6252\n",
      "Epoch 63/100\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 1.3050 - accuracy: 0.6237\n",
      "Epoch 64/100\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.2746 - accuracy: 0.6295\n",
      "Epoch 65/100\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.2417 - accuracy: 0.6266\n",
      "Epoch 66/100\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 1.2110 - accuracy: 0.6237\n",
      "Epoch 67/100\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.1922 - accuracy: 0.6252\n",
      "Epoch 68/100\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 1.1809 - accuracy: 0.6411\n",
      "Epoch 69/100\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.1358 - accuracy: 0.6339\n",
      "Epoch 70/100\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 1.1066 - accuracy: 0.6295\n",
      "Epoch 71/100\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 1.1033 - accuracy: 0.6295\n",
      "Epoch 72/100\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 1.0383 - accuracy: 0.6295\n",
      "Epoch 73/100\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 1.0341 - accuracy: 0.6368\n",
      "Epoch 74/100\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9824 - accuracy: 0.6281\n",
      "Epoch 75/100\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.9757 - accuracy: 0.6425\n",
      "Epoch 76/100\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.9617 - accuracy: 0.6469\n",
      "Epoch 77/100\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.9266 - accuracy: 0.6614\n",
      "Epoch 78/100\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.9240 - accuracy: 0.6454\n",
      "Epoch 79/100\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.9035 - accuracy: 0.6454\n",
      "Epoch 80/100\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.8782 - accuracy: 0.6368\n",
      "Epoch 81/100\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8509 - accuracy: 0.6628\n",
      "Epoch 82/100\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.8371 - accuracy: 0.6512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/100\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.8217 - accuracy: 0.6657\n",
      "Epoch 84/100\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.8356 - accuracy: 0.6527\n",
      "Epoch 85/100\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.8266 - accuracy: 0.6599\n",
      "Epoch 86/100\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.8103 - accuracy: 0.6498\n",
      "Epoch 87/100\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.7903 - accuracy: 0.6556\n",
      "Epoch 88/100\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.7709 - accuracy: 0.6643\n",
      "Epoch 89/100\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.7537 - accuracy: 0.6628\n",
      "Epoch 90/100\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7383 - accuracy: 0.6671\n",
      "Epoch 91/100\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.7425 - accuracy: 0.6729\n",
      "Epoch 92/100\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.7399 - accuracy: 0.6686\n",
      "Epoch 93/100\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.7323 - accuracy: 0.6570\n",
      "Epoch 94/100\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.7140 - accuracy: 0.6773\n",
      "Epoch 95/100\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.7130 - accuracy: 0.6643\n",
      "Epoch 96/100\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.6882 - accuracy: 0.6860\n",
      "Epoch 97/100\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6920 - accuracy: 0.6729\n",
      "Epoch 98/100\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6779 - accuracy: 0.6831\n",
      "Epoch 99/100\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.6714 - accuracy: 0.6773\n",
      "Epoch 100/100\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6649 - accuracy: 0.6773\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26b468faf40>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#and start training\n",
    "model.fit(X_train,\n",
    "          y_train,\n",
    "          epochs=100, \n",
    "          batch_size=20, \n",
    "          verbose=1\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.core.dense.Dense at 0x26b46402be0>,\n",
       " <keras.layers.core.dense.Dense at 0x26b469611c0>,\n",
       " <keras.layers.core.dense.Dense at 0x26b4691e670>]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\admin\\\\Desktop\\\\Rahul\\\\git\\\\DataSciencePython\\\\DeepLearning\\\\MLP'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "\n",
    "plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ValueError: Data cardinality is ambiguous:\n",
    "  x sizes: 691\n",
    "  y sizes: 2\n",
    "Make sure all arrays contain the same number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ValueError: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {'(<class \\'list\\'> containing values of types {\"<class \\'float\\'>\"})'}), <class 'numpy.ndarray'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For Each Attribute: (all numeric-valued)\n",
    "   1. Number of times pregnant\n",
    "   2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "   3. Diastolic blood pressure (mm Hg)\n",
    "   4. Triceps skin fold thickness (mm)\n",
    "   5. 2-Hour serum insulin (mu U/ml)\n",
    "   6. Body mass index (weight in kg/(height in m)^2)\n",
    "   7. Diabetes pedigree function\n",
    "   8. Age (years)\n",
    "   \n",
    "   \n",
    "   9. Class variable (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.   , 148.   ,  72.   , ...,   0.627,  50.   ,   1.   ],\n",
       "       [  1.   ,  85.   ,  66.   , ...,   0.351,  31.   ,   0.   ],\n",
       "       [  8.   , 183.   ,  64.   , ...,   0.672,  32.   ,   1.   ],\n",
       "       ...,\n",
       "       [  5.   , 121.   ,  72.   , ...,   0.245,  30.   ,   0.   ],\n",
       "       [  1.   , 126.   ,  60.   , ...,   0.349,  47.   ,   1.   ],\n",
       "       [  1.   ,  93.   ,  70.   , ...,   0.315,  23.   ,   0.   ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import loadtxt\n",
    "# load the dataset\n",
    "\n",
    "dataset = loadtxt('diabetes.csv', delimiter=',')\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "5    0\n",
       "6    0\n",
       "7    0\n",
       "8    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(dataset).isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dataset).duplicated().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 9)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = dataset[:,0:8]\n",
    "\n",
    "y = dataset[:,8]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
       "       0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "       1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
       "       1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "       1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
       "       1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
       "       0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
       "       1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
       "       1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
       "       1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
       "       1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
       "       1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1.,\n",
       "       1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0.,\n",
       "       0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
       "       1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1.,\n",
       "       1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1.,\n",
       "       1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
       "       0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
       "       0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
       "       0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
       "       1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
       "       0., 1., 0.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=100,test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.000e+00, 7.900e+01, 6.000e+01, 4.200e+01, 4.800e+01, 4.350e+01,\n",
       "        6.780e-01, 2.300e+01],\n",
       "       [0.000e+00, 8.600e+01, 6.800e+01, 3.200e+01, 0.000e+00, 3.580e+01,\n",
       "        2.380e-01, 2.500e+01],\n",
       "       [5.000e+00, 1.620e+02, 1.040e+02, 0.000e+00, 0.000e+00, 3.770e+01,\n",
       "        1.510e-01, 5.200e+01],\n",
       "       [8.000e+00, 6.500e+01, 7.200e+01, 2.300e+01, 0.000e+00, 3.200e+01,\n",
       "        6.000e-01, 4.200e+01],\n",
       "       [9.000e+00, 1.230e+02, 7.000e+01, 4.400e+01, 9.400e+01, 3.310e+01,\n",
       "        3.740e-01, 4.000e+01],\n",
       "       [8.000e+00, 1.540e+02, 7.800e+01, 3.200e+01, 0.000e+00, 3.240e+01,\n",
       "        4.430e-01, 4.500e+01],\n",
       "       [1.700e+01, 1.630e+02, 7.200e+01, 4.100e+01, 1.140e+02, 4.090e+01,\n",
       "        8.170e-01, 4.700e+01],\n",
       "       [0.000e+00, 1.040e+02, 6.400e+01, 3.700e+01, 6.400e+01, 3.360e+01,\n",
       "        5.100e-01, 2.200e+01],\n",
       "       [1.000e+00, 1.960e+02, 7.600e+01, 3.600e+01, 2.490e+02, 3.650e+01,\n",
       "        8.750e-01, 2.900e+01],\n",
       "       [1.000e+00, 1.170e+02, 6.000e+01, 2.300e+01, 1.060e+02, 3.380e+01,\n",
       "        4.660e-01, 2.700e+01],\n",
       "       [4.000e+00, 8.400e+01, 9.000e+01, 2.300e+01, 5.600e+01, 3.950e+01,\n",
       "        1.590e-01, 2.500e+01],\n",
       "       [8.000e+00, 1.790e+02, 7.200e+01, 4.200e+01, 1.300e+02, 3.270e+01,\n",
       "        7.190e-01, 3.600e+01],\n",
       "       [1.000e+00, 1.200e+02, 8.000e+01, 4.800e+01, 2.000e+02, 3.890e+01,\n",
       "        1.162e+00, 4.100e+01],\n",
       "       [6.000e+00, 1.050e+02, 8.000e+01, 2.800e+01, 0.000e+00, 3.250e+01,\n",
       "        8.780e-01, 2.600e+01],\n",
       "       [3.000e+00, 1.800e+02, 6.400e+01, 2.500e+01, 7.000e+01, 3.400e+01,\n",
       "        2.710e-01, 2.600e+01],\n",
       "       [1.200e+01, 8.800e+01, 7.400e+01, 4.000e+01, 5.400e+01, 3.530e+01,\n",
       "        3.780e-01, 4.800e+01],\n",
       "       [2.000e+00, 9.900e+01, 5.200e+01, 1.500e+01, 9.400e+01, 2.460e+01,\n",
       "        6.370e-01, 2.100e+01],\n",
       "       [1.000e+00, 1.150e+02, 7.000e+01, 3.000e+01, 9.600e+01, 3.460e+01,\n",
       "        5.290e-01, 3.200e+01],\n",
       "       [0.000e+00, 1.400e+02, 6.500e+01, 2.600e+01, 1.300e+02, 4.260e+01,\n",
       "        4.310e-01, 2.400e+01],\n",
       "       [1.100e+01, 8.500e+01, 7.400e+01, 0.000e+00, 0.000e+00, 3.010e+01,\n",
       "        3.000e-01, 3.500e+01],\n",
       "       [4.000e+00, 1.290e+02, 8.600e+01, 2.000e+01, 2.700e+02, 3.510e+01,\n",
       "        2.310e-01, 2.300e+01],\n",
       "       [1.000e+00, 7.100e+01, 4.800e+01, 1.800e+01, 7.600e+01, 2.040e+01,\n",
       "        3.230e-01, 2.200e+01],\n",
       "       [5.000e+00, 1.440e+02, 8.200e+01, 2.600e+01, 2.850e+02, 3.200e+01,\n",
       "        4.520e-01, 5.800e+01],\n",
       "       [5.000e+00, 1.870e+02, 7.600e+01, 2.700e+01, 2.070e+02, 4.360e+01,\n",
       "        1.034e+00, 5.300e+01],\n",
       "       [3.000e+00, 9.000e+01, 7.800e+01, 0.000e+00, 0.000e+00, 4.270e+01,\n",
       "        5.590e-01, 2.100e+01],\n",
       "       [1.300e+01, 1.260e+02, 9.000e+01, 0.000e+00, 0.000e+00, 4.340e+01,\n",
       "        5.830e-01, 4.200e+01],\n",
       "       [1.000e+00, 1.390e+02, 6.200e+01, 4.100e+01, 4.800e+02, 4.070e+01,\n",
       "        5.360e-01, 2.100e+01],\n",
       "       [5.000e+00, 1.260e+02, 7.800e+01, 2.700e+01, 2.200e+01, 2.960e+01,\n",
       "        4.390e-01, 4.000e+01],\n",
       "       [1.000e+00, 8.800e+01, 3.000e+01, 4.200e+01, 9.900e+01, 5.500e+01,\n",
       "        4.960e-01, 2.600e+01],\n",
       "       [4.000e+00, 1.450e+02, 8.200e+01, 1.800e+01, 0.000e+00, 3.250e+01,\n",
       "        2.350e-01, 7.000e+01],\n",
       "       [1.000e+00, 8.000e+01, 7.400e+01, 1.100e+01, 6.000e+01, 3.000e+01,\n",
       "        5.270e-01, 2.200e+01],\n",
       "       [8.000e+00, 1.960e+02, 7.600e+01, 2.900e+01, 2.800e+02, 3.750e+01,\n",
       "        6.050e-01, 5.700e+01],\n",
       "       [1.000e+00, 7.700e+01, 5.600e+01, 3.000e+01, 5.600e+01, 3.330e+01,\n",
       "        1.251e+00, 2.400e+01],\n",
       "       [9.000e+00, 1.340e+02, 7.400e+01, 3.300e+01, 6.000e+01, 2.590e+01,\n",
       "        4.600e-01, 8.100e+01],\n",
       "       [5.000e+00, 8.500e+01, 7.400e+01, 2.200e+01, 0.000e+00, 2.900e+01,\n",
       "        1.224e+00, 3.200e+01],\n",
       "       [5.000e+00, 1.140e+02, 7.400e+01, 0.000e+00, 0.000e+00, 2.490e+01,\n",
       "        7.440e-01, 5.700e+01],\n",
       "       [7.000e+00, 1.870e+02, 5.000e+01, 3.300e+01, 3.920e+02, 3.390e+01,\n",
       "        8.260e-01, 3.400e+01],\n",
       "       [0.000e+00, 1.050e+02, 6.800e+01, 2.200e+01, 0.000e+00, 2.000e+01,\n",
       "        2.360e-01, 2.200e+01],\n",
       "       [2.000e+00, 1.020e+02, 8.600e+01, 3.600e+01, 1.200e+02, 4.550e+01,\n",
       "        1.270e-01, 2.300e+01],\n",
       "       [3.000e+00, 1.210e+02, 5.200e+01, 0.000e+00, 0.000e+00, 3.600e+01,\n",
       "        1.270e-01, 2.500e+01],\n",
       "       [7.000e+00, 1.030e+02, 6.600e+01, 3.200e+01, 0.000e+00, 3.910e+01,\n",
       "        3.440e-01, 3.100e+01],\n",
       "       [6.000e+00, 1.650e+02, 6.800e+01, 2.600e+01, 1.680e+02, 3.360e+01,\n",
       "        6.310e-01, 4.900e+01],\n",
       "       [8.000e+00, 1.000e+02, 7.400e+01, 4.000e+01, 2.150e+02, 3.940e+01,\n",
       "        6.610e-01, 4.300e+01],\n",
       "       [6.000e+00, 1.030e+02, 6.600e+01, 0.000e+00, 0.000e+00, 2.430e+01,\n",
       "        2.490e-01, 2.900e+01],\n",
       "       [6.000e+00, 1.140e+02, 8.800e+01, 0.000e+00, 0.000e+00, 2.780e+01,\n",
       "        2.470e-01, 6.600e+01],\n",
       "       [6.000e+00, 1.900e+02, 9.200e+01, 0.000e+00, 0.000e+00, 3.550e+01,\n",
       "        2.780e-01, 6.600e+01],\n",
       "       [2.000e+00, 1.300e+02, 9.600e+01, 0.000e+00, 0.000e+00, 2.260e+01,\n",
       "        2.680e-01, 2.100e+01],\n",
       "       [4.000e+00, 1.100e+02, 9.200e+01, 0.000e+00, 0.000e+00, 3.760e+01,\n",
       "        1.910e-01, 3.000e+01],\n",
       "       [4.000e+00, 1.710e+02, 7.200e+01, 0.000e+00, 0.000e+00, 4.360e+01,\n",
       "        4.790e-01, 2.600e+01],\n",
       "       [1.000e+01, 1.790e+02, 7.000e+01, 0.000e+00, 0.000e+00, 3.510e+01,\n",
       "        2.000e-01, 3.700e+01],\n",
       "       [1.000e+01, 6.800e+01, 1.060e+02, 2.300e+01, 4.900e+01, 3.550e+01,\n",
       "        2.850e-01, 4.700e+01],\n",
       "       [0.000e+00, 1.040e+02, 7.600e+01, 0.000e+00, 0.000e+00, 1.840e+01,\n",
       "        5.820e-01, 2.700e+01],\n",
       "       [5.000e+00, 1.390e+02, 8.000e+01, 3.500e+01, 1.600e+02, 3.160e+01,\n",
       "        3.610e-01, 2.500e+01],\n",
       "       [0.000e+00, 1.790e+02, 5.000e+01, 3.600e+01, 1.590e+02, 3.780e+01,\n",
       "        4.550e-01, 2.200e+01],\n",
       "       [7.000e+00, 1.330e+02, 8.400e+01, 0.000e+00, 0.000e+00, 4.020e+01,\n",
       "        6.960e-01, 3.700e+01],\n",
       "       [9.000e+00, 1.560e+02, 8.600e+01, 2.800e+01, 1.550e+02, 3.430e+01,\n",
       "        1.189e+00, 4.200e+01],\n",
       "       [3.000e+00, 1.070e+02, 6.200e+01, 1.300e+01, 4.800e+01, 2.290e+01,\n",
       "        6.780e-01, 2.300e+01],\n",
       "       [5.000e+00, 8.800e+01, 7.800e+01, 3.000e+01, 0.000e+00, 2.760e+01,\n",
       "        2.580e-01, 3.700e+01],\n",
       "       [3.000e+00, 1.930e+02, 7.000e+01, 3.100e+01, 0.000e+00, 3.490e+01,\n",
       "        2.410e-01, 2.500e+01],\n",
       "       [6.000e+00, 9.300e+01, 5.000e+01, 3.000e+01, 6.400e+01, 2.870e+01,\n",
       "        3.560e-01, 2.300e+01],\n",
       "       [2.000e+00, 1.120e+02, 6.800e+01, 2.200e+01, 9.400e+01, 3.410e+01,\n",
       "        3.150e-01, 2.600e+01],\n",
       "       [1.000e+00, 1.080e+02, 6.000e+01, 4.600e+01, 1.780e+02, 3.550e+01,\n",
       "        4.150e-01, 2.400e+01],\n",
       "       [3.000e+00, 1.280e+02, 7.200e+01, 2.500e+01, 1.900e+02, 3.240e+01,\n",
       "        5.490e-01, 2.700e+01],\n",
       "       [1.000e+00, 1.240e+02, 6.000e+01, 3.200e+01, 0.000e+00, 3.580e+01,\n",
       "        5.140e-01, 2.100e+01],\n",
       "       [8.000e+00, 1.120e+02, 7.200e+01, 0.000e+00, 0.000e+00, 2.360e+01,\n",
       "        8.400e-01, 5.800e+01],\n",
       "       [2.000e+00, 1.220e+02, 6.000e+01, 1.800e+01, 1.060e+02, 2.980e+01,\n",
       "        7.170e-01, 2.200e+01],\n",
       "       [2.000e+00, 1.150e+02, 6.400e+01, 2.200e+01, 0.000e+00, 3.080e+01,\n",
       "        4.210e-01, 2.100e+01],\n",
       "       [1.000e+00, 8.100e+01, 7.200e+01, 1.800e+01, 4.000e+01, 2.660e+01,\n",
       "        2.830e-01, 2.400e+01],\n",
       "       [0.000e+00, 1.130e+02, 8.000e+01, 1.600e+01, 0.000e+00, 3.100e+01,\n",
       "        8.740e-01, 2.100e+01],\n",
       "       [2.000e+00, 9.000e+01, 8.000e+01, 1.400e+01, 5.500e+01, 2.440e+01,\n",
       "        2.490e-01, 2.400e+01],\n",
       "       [0.000e+00, 7.300e+01, 0.000e+00, 0.000e+00, 0.000e+00, 2.110e+01,\n",
       "        3.420e-01, 2.500e+01],\n",
       "       [1.100e+01, 1.430e+02, 9.400e+01, 3.300e+01, 1.460e+02, 3.660e+01,\n",
       "        2.540e-01, 5.100e+01],\n",
       "       [0.000e+00, 1.370e+02, 6.800e+01, 1.400e+01, 1.480e+02, 2.480e+01,\n",
       "        1.430e-01, 2.100e+01],\n",
       "       [0.000e+00, 1.190e+02, 6.600e+01, 2.700e+01, 0.000e+00, 3.880e+01,\n",
       "        2.590e-01, 2.200e+01],\n",
       "       [1.200e+01, 8.400e+01, 7.200e+01, 3.100e+01, 0.000e+00, 2.970e+01,\n",
       "        2.970e-01, 4.600e+01],\n",
       "       [0.000e+00, 1.270e+02, 8.000e+01, 3.700e+01, 2.100e+02, 3.630e+01,\n",
       "        8.040e-01, 2.300e+01],\n",
       "       [7.000e+00, 1.190e+02, 0.000e+00, 0.000e+00, 0.000e+00, 2.520e+01,\n",
       "        2.090e-01, 3.700e+01]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
       "       1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
       "       0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "\n",
    "model.add(Dense(8, activation='relu'))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "\n",
    "model2.add(Dense(12, input_dim=8, activation='relu'))\n",
    "\n",
    "model2.add(Dense(8, activation='relu'))\n",
    "\n",
    "model2.add(Dense(1, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model2.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(691, 8)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential_4\" is incompatible with the layer: expected shape=(None, 2), found shape=(None, 8)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12156\\2352504795.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m model.fit(X_train,\n\u001b[0m\u001b[0;32m      2\u001b[0m           \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m           batch_size=100)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                     \u001b[0mretval_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                 \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential_4\" is incompatible with the layer: expected shape=(None, 2), found shape=(None, 8)\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train,\n",
    "          y_train, \n",
    "          epochs=150, \n",
    "          batch_size=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "70/70 [==============================] - 1s 1ms/step - loss: 0.8957 - accuracy: 0.3444\n",
      "Epoch 2/150\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.7641 - accuracy: 0.3444\n",
      "Epoch 3/150\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.6999 - accuracy: 0.3444\n",
      "Epoch 4/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.6967 - accuracy: 0.3444\n",
      "Epoch 5/150\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.6370 - accuracy: 0.3444\n",
      "Epoch 6/150\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.6329 - accuracy: 0.3444\n",
      "Epoch 7/150\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.6224 - accuracy: 0.3444\n",
      "Epoch 8/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.6073 - accuracy: 0.3444\n",
      "Epoch 9/150\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.6104 - accuracy: 0.3444\n",
      "Epoch 10/150\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.6140 - accuracy: 0.3444\n",
      "Epoch 11/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.6226 - accuracy: 0.3444\n",
      "Epoch 12/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.6095 - accuracy: 0.3444\n",
      "Epoch 13/150\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.6056 - accuracy: 0.3444\n",
      "Epoch 14/150\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.6053 - accuracy: 0.3444\n",
      "Epoch 15/150\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.5933 - accuracy: 0.3444\n",
      "Epoch 16/150\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.5999 - accuracy: 0.3444\n",
      "Epoch 17/150\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.5898 - accuracy: 0.3444\n",
      "Epoch 18/150\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.5857 - accuracy: 0.3444\n",
      "Epoch 19/150\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.5864 - accuracy: 0.3444\n",
      "Epoch 20/150\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.6037 - accuracy: 0.3444\n",
      "Epoch 21/150\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.5835 - accuracy: 0.3444\n",
      "Epoch 22/150\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.5783 - accuracy: 0.3444\n",
      "Epoch 23/150\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.5750 - accuracy: 0.3444\n",
      "Epoch 24/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5834 - accuracy: 0.3444\n",
      "Epoch 25/150\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.5782 - accuracy: 0.3444\n",
      "Epoch 26/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5735 - accuracy: 0.3444\n",
      "Epoch 27/150\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.5719 - accuracy: 0.3444\n",
      "Epoch 28/150\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.5786 - accuracy: 0.3444\n",
      "Epoch 29/150\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.5748 - accuracy: 0.3444\n",
      "Epoch 30/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5648 - accuracy: 0.3444\n",
      "Epoch 31/150\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.5723 - accuracy: 0.3444\n",
      "Epoch 32/150\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.5764 - accuracy: 0.3444\n",
      "Epoch 33/150\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.5680 - accuracy: 0.3444\n",
      "Epoch 34/150\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.5739 - accuracy: 0.3444\n",
      "Epoch 35/150\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.5661 - accuracy: 0.3444\n",
      "Epoch 36/150\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.5613 - accuracy: 0.3444\n",
      "Epoch 37/150\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.5586 - accuracy: 0.3444\n",
      "Epoch 38/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5611 - accuracy: 0.3444\n",
      "Epoch 39/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5624 - accuracy: 0.3444\n",
      "Epoch 40/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5582 - accuracy: 0.3444\n",
      "Epoch 41/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5569 - accuracy: 0.3444\n",
      "Epoch 42/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5563 - accuracy: 0.3444\n",
      "Epoch 43/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5576 - accuracy: 0.3444\n",
      "Epoch 44/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5565 - accuracy: 0.3444\n",
      "Epoch 45/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5502 - accuracy: 0.3444\n",
      "Epoch 46/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5564 - accuracy: 0.3444\n",
      "Epoch 47/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5532 - accuracy: 0.3444\n",
      "Epoch 48/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5500 - accuracy: 0.3444\n",
      "Epoch 49/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5554 - accuracy: 0.3444\n",
      "Epoch 50/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5462 - accuracy: 0.3444\n",
      "Epoch 51/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5586 - accuracy: 0.3444\n",
      "Epoch 52/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5511 - accuracy: 0.3444\n",
      "Epoch 53/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5579 - accuracy: 0.3444\n",
      "Epoch 54/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5626 - accuracy: 0.3444\n",
      "Epoch 55/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5579 - accuracy: 0.3444\n",
      "Epoch 56/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5479 - accuracy: 0.3444\n",
      "Epoch 57/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5443 - accuracy: 0.3444\n",
      "Epoch 58/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5435 - accuracy: 0.3444\n",
      "Epoch 59/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5415 - accuracy: 0.3444\n",
      "Epoch 60/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5443 - accuracy: 0.3444\n",
      "Epoch 61/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5401 - accuracy: 0.3444\n",
      "Epoch 62/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5377 - accuracy: 0.3444\n",
      "Epoch 63/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5414 - accuracy: 0.3444\n",
      "Epoch 64/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5351 - accuracy: 0.3444\n",
      "Epoch 65/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5384 - accuracy: 0.3444\n",
      "Epoch 66/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5359 - accuracy: 0.3444\n",
      "Epoch 67/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5329 - accuracy: 0.3444\n",
      "Epoch 68/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5356 - accuracy: 0.3444\n",
      "Epoch 69/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5449 - accuracy: 0.3444\n",
      "Epoch 70/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5369 - accuracy: 0.3444\n",
      "Epoch 71/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5391 - accuracy: 0.3444\n",
      "Epoch 72/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5287 - accuracy: 0.3444\n",
      "Epoch 73/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5335 - accuracy: 0.3444\n",
      "Epoch 74/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5344 - accuracy: 0.3444\n",
      "Epoch 75/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5263 - accuracy: 0.3444\n",
      "Epoch 76/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5319 - accuracy: 0.3444\n",
      "Epoch 77/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5239 - accuracy: 0.3444\n",
      "Epoch 78/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5259 - accuracy: 0.3444\n",
      "Epoch 79/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5289 - accuracy: 0.3444\n",
      "Epoch 80/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5325 - accuracy: 0.3444\n",
      "Epoch 81/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5247 - accuracy: 0.3444\n",
      "Epoch 82/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5378 - accuracy: 0.3444\n",
      "Epoch 83/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5227 - accuracy: 0.3444\n",
      "Epoch 84/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5216 - accuracy: 0.3444\n",
      "Epoch 85/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5364 - accuracy: 0.3444\n",
      "Epoch 86/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5321 - accuracy: 0.3444\n",
      "Epoch 87/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5351 - accuracy: 0.3444\n",
      "Epoch 88/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5168 - accuracy: 0.3444\n",
      "Epoch 89/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5168 - accuracy: 0.3444\n",
      "Epoch 90/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5254 - accuracy: 0.3444\n",
      "Epoch 91/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5184 - accuracy: 0.3444\n",
      "Epoch 92/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5152 - accuracy: 0.3444\n",
      "Epoch 93/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5135 - accuracy: 0.3444\n",
      "Epoch 94/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5208 - accuracy: 0.3444\n",
      "Epoch 95/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5140 - accuracy: 0.3444\n",
      "Epoch 96/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5109 - accuracy: 0.3444\n",
      "Epoch 97/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5142 - accuracy: 0.3444\n",
      "Epoch 98/150\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.5096 - accuracy: 0.3444\n",
      "Epoch 99/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5187 - accuracy: 0.3444\n",
      "Epoch 100/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5157 - accuracy: 0.3444\n",
      "Epoch 101/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5152 - accuracy: 0.3444\n",
      "Epoch 102/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5179 - accuracy: 0.3444\n",
      "Epoch 103/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5064 - accuracy: 0.3444\n",
      "Epoch 104/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5051 - accuracy: 0.3444\n",
      "Epoch 105/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5027 - accuracy: 0.3444\n",
      "Epoch 106/150\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.5196 - accuracy: 0.3444\n",
      "Epoch 107/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5063 - accuracy: 0.3444\n",
      "Epoch 108/150\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.5076 - accuracy: 0.3444\n",
      "Epoch 109/150\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.5008 - accuracy: 0.3444\n",
      "Epoch 110/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5064 - accuracy: 0.3444\n",
      "Epoch 111/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5102 - accuracy: 0.3444\n",
      "Epoch 112/150\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.5048 - accuracy: 0.3444\n",
      "Epoch 113/150\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.5025 - accuracy: 0.3444\n",
      "Epoch 114/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4984 - accuracy: 0.3444\n",
      "Epoch 115/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4968 - accuracy: 0.3444\n",
      "Epoch 116/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5039 - accuracy: 0.3444\n",
      "Epoch 117/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5058 - accuracy: 0.3444\n",
      "Epoch 118/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4937 - accuracy: 0.3444\n",
      "Epoch 119/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5029 - accuracy: 0.3444\n",
      "Epoch 120/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4931 - accuracy: 0.3444\n",
      "Epoch 121/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5014 - accuracy: 0.3444\n",
      "Epoch 122/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5012 - accuracy: 0.3444\n",
      "Epoch 123/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4958 - accuracy: 0.3444\n",
      "Epoch 124/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5000 - accuracy: 0.3444\n",
      "Epoch 125/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4929 - accuracy: 0.3444\n",
      "Epoch 126/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4896 - accuracy: 0.3444\n",
      "Epoch 127/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5107 - accuracy: 0.3444\n",
      "Epoch 128/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5055 - accuracy: 0.3444\n",
      "Epoch 129/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5031 - accuracy: 0.3444\n",
      "Epoch 130/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4945 - accuracy: 0.3444\n",
      "Epoch 131/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4893 - accuracy: 0.3444\n",
      "Epoch 132/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4987 - accuracy: 0.3444\n",
      "Epoch 133/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4930 - accuracy: 0.3444\n",
      "Epoch 134/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4927 - accuracy: 0.3444\n",
      "Epoch 135/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4879 - accuracy: 0.3444\n",
      "Epoch 136/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4860 - accuracy: 0.3444\n",
      "Epoch 137/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4961 - accuracy: 0.3444\n",
      "Epoch 138/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4885 - accuracy: 0.3444\n",
      "Epoch 139/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5022 - accuracy: 0.3444\n",
      "Epoch 140/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4851 - accuracy: 0.3444\n",
      "Epoch 141/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4848 - accuracy: 0.3444\n",
      "Epoch 142/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4842 - accuracy: 0.3444\n",
      "Epoch 143/150\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.4863 - accuracy: 0.3444\n",
      "Epoch 144/150\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.4806 - accuracy: 0.3444\n",
      "Epoch 145/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4788 - accuracy: 0.3444\n",
      "Epoch 146/150\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.4910 - accuracy: 0.3444\n",
      "Epoch 147/150\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.4821 - accuracy: 0.3444\n",
      "Epoch 148/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4821 - accuracy: 0.3444\n",
      "Epoch 149/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4796 - accuracy: 0.3444\n",
      "Epoch 150/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4719 - accuracy: 0.3444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26b4191dcd0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(X_train, \n",
    "           y_train, \n",
    "           epochs=150, \n",
    "           batch_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 12)                108       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 8)                 104       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 221\n",
      "Trainable params: 221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 12)                108       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 8)                 104       \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 221\n",
      "Trainable params: 221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_3/kernel:0' shape=(8, 12) dtype=float32, numpy=\n",
       " array([[-0.2707341 , -0.05080863,  0.3325677 ,  0.02661689,  0.09546687,\n",
       "         -0.1873606 , -0.56445783, -0.07520852, -0.2195475 ,  0.24724407,\n",
       "          0.29515406,  0.14113897],\n",
       "        [-0.2843238 ,  0.42958272, -0.19047323,  0.5046283 , -0.42128384,\n",
       "         -0.15907434, -0.5476401 , -0.13845648, -0.42744014,  0.27018133,\n",
       "          0.30733407, -0.41038227],\n",
       "        [-0.2177605 ,  0.06441411,  0.05467284,  0.44065064, -0.21274287,\n",
       "         -0.49283966, -0.09673377,  0.42608175,  0.4570028 , -0.5002235 ,\n",
       "          0.31202394, -0.51010615],\n",
       "        [ 0.24166006,  0.40772468, -0.32236308,  0.12306885, -0.2525902 ,\n",
       "         -0.21571778,  0.03963589, -0.20910929, -0.12102606, -0.18341671,\n",
       "          0.29361644,  0.51757085],\n",
       "        [-0.48086348,  0.03351846, -0.5117834 ,  0.5164131 ,  0.56760657,\n",
       "          0.04628246, -0.06292452,  0.4255371 , -0.50929636,  0.3737991 ,\n",
       "          0.37157026, -0.0203284 ],\n",
       "        [-0.4640039 , -0.21122034, -0.01814133,  0.38055998, -0.02454132,\n",
       "          0.19957982,  0.12898745,  0.06304277, -0.5353373 , -0.12106156,\n",
       "         -0.27515098, -0.40937024],\n",
       "        [ 0.16720873, -0.13518979,  0.51126194,  0.34953305,  0.42334765,\n",
       "          0.47999576,  0.11757132, -0.31820905,  0.05699358, -0.4340453 ,\n",
       "         -0.4316951 , -0.28483924],\n",
       "        [-0.09278002, -0.01127183, -0.32012194,  0.3983644 , -0.00153066,\n",
       "          0.37774208,  0.3449576 ,  0.1327067 , -0.43586415,  0.01484597,\n",
       "          0.0556513 ,  0.3025329 ]], dtype=float32)>,\n",
       " <tf.Variable 'dense_3/bias:0' shape=(12,) dtype=float32, numpy=\n",
       " array([ 0.        ,  0.19300531,  0.        , -0.17343715,  0.18361388,\n",
       "         0.06771387, -0.22421587,  0.452888  , -0.08132499, -0.21666393,\n",
       "        -0.07544699,  0.        ], dtype=float32)>,\n",
       " <tf.Variable 'dense_4/kernel:0' shape=(12, 8) dtype=float32, numpy=\n",
       " array([[-0.3113039 ,  0.17023903, -0.344167  ,  0.29333562, -0.02136618,\n",
       "         -0.42640874,  0.07107395, -0.303716  ],\n",
       "        [ 0.4666558 , -0.09738466,  0.28561953, -0.5307702 ,  0.09173138,\n",
       "         -0.05101156,  0.36804244,  0.3019262 ],\n",
       "        [ 0.41733384,  0.49172986,  0.20637679, -0.04217017, -0.39245188,\n",
       "         -0.19149938,  0.0083636 , -0.3909765 ],\n",
       "        [-0.34030655, -0.4450272 ,  0.39405745,  0.24152023,  0.16998816,\n",
       "         -0.42297208,  0.13509683, -0.14254525],\n",
       "        [ 0.01523143,  0.13871175,  0.05670753, -0.40592638,  0.16226704,\n",
       "         -0.09528312, -0.5008731 , -0.4883293 ],\n",
       "        [ 0.30663335, -0.03179562,  0.10163467, -0.27459598, -0.545521  ,\n",
       "         -0.45532885, -0.6964867 ,  0.49242485],\n",
       "        [ 0.5048044 , -0.01427656,  0.10933889,  0.19139433,  0.22286911,\n",
       "          0.24838883, -0.19801924,  0.11525041],\n",
       "        [ 0.43146405, -0.26046214,  0.363012  ,  0.01449148, -0.00475719,\n",
       "         -0.38059026, -0.33750382,  0.33250004],\n",
       "        [-0.25415793, -0.23779818,  0.34854904, -0.28544727,  0.4335922 ,\n",
       "         -0.12572679,  0.4125948 , -0.03685588],\n",
       "        [-0.3317091 ,  0.03989869,  0.14365958, -0.36386275,  0.382919  ,\n",
       "         -0.41627803, -0.4040991 , -0.08117577],\n",
       "        [-0.27333277,  0.09415042, -0.03617074,  0.44426197, -0.11926705,\n",
       "          0.36968815, -0.3668377 , -0.28041625],\n",
       "        [ 0.09219563, -0.52739406, -0.40611184,  0.44082528, -0.5018964 ,\n",
       "         -0.45023698,  0.18786281, -0.12455985]], dtype=float32)>,\n",
       " <tf.Variable 'dense_4/bias:0' shape=(8,) dtype=float32, numpy=\n",
       " array([-0.03775376,  0.        ,  0.2708472 , -0.22116502, -0.25540894,\n",
       "         0.        , -0.33988944,  0.        ], dtype=float32)>,\n",
       " <tf.Variable 'dense_5/kernel:0' shape=(8, 1) dtype=float32, numpy=\n",
       " array([[ 0.7070635 ],\n",
       "        [-0.31568348],\n",
       "        [-0.376311  ],\n",
       "        [ 0.3826491 ],\n",
       "        [ 0.70751894],\n",
       "        [ 0.5162066 ],\n",
       "        [ 0.31639606],\n",
       "        [-0.507975  ]], dtype=float32)>,\n",
       " <tf.Variable 'dense_5/bias:0' shape=(1,) dtype=float32, numpy=array([-0.26016304], dtype=float32)>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_3/kernel:0' shape=(8, 12) dtype=float32, numpy=\n",
       " array([[-0.2707341 , -0.05080863,  0.3325677 ,  0.02661689,  0.09546687,\n",
       "         -0.1873606 , -0.56445783, -0.07520852, -0.2195475 ,  0.24724407,\n",
       "          0.29515406,  0.14113897],\n",
       "        [-0.2843238 ,  0.42958272, -0.19047323,  0.5046283 , -0.42128384,\n",
       "         -0.15907434, -0.5476401 , -0.13845648, -0.42744014,  0.27018133,\n",
       "          0.30733407, -0.41038227],\n",
       "        [-0.2177605 ,  0.06441411,  0.05467284,  0.44065064, -0.21274287,\n",
       "         -0.49283966, -0.09673377,  0.42608175,  0.4570028 , -0.5002235 ,\n",
       "          0.31202394, -0.51010615],\n",
       "        [ 0.24166006,  0.40772468, -0.32236308,  0.12306885, -0.2525902 ,\n",
       "         -0.21571778,  0.03963589, -0.20910929, -0.12102606, -0.18341671,\n",
       "          0.29361644,  0.51757085],\n",
       "        [-0.48086348,  0.03351846, -0.5117834 ,  0.5164131 ,  0.56760657,\n",
       "          0.04628246, -0.06292452,  0.4255371 , -0.50929636,  0.3737991 ,\n",
       "          0.37157026, -0.0203284 ],\n",
       "        [-0.4640039 , -0.21122034, -0.01814133,  0.38055998, -0.02454132,\n",
       "          0.19957982,  0.12898745,  0.06304277, -0.5353373 , -0.12106156,\n",
       "         -0.27515098, -0.40937024],\n",
       "        [ 0.16720873, -0.13518979,  0.51126194,  0.34953305,  0.42334765,\n",
       "          0.47999576,  0.11757132, -0.31820905,  0.05699358, -0.4340453 ,\n",
       "         -0.4316951 , -0.28483924],\n",
       "        [-0.09278002, -0.01127183, -0.32012194,  0.3983644 , -0.00153066,\n",
       "          0.37774208,  0.3449576 ,  0.1327067 , -0.43586415,  0.01484597,\n",
       "          0.0556513 ,  0.3025329 ]], dtype=float32)>,\n",
       " <tf.Variable 'dense_3/bias:0' shape=(12,) dtype=float32, numpy=\n",
       " array([ 0.        ,  0.19300531,  0.        , -0.17343715,  0.18361388,\n",
       "         0.06771387, -0.22421587,  0.452888  , -0.08132499, -0.21666393,\n",
       "        -0.07544699,  0.        ], dtype=float32)>,\n",
       " <tf.Variable 'dense_4/kernel:0' shape=(12, 8) dtype=float32, numpy=\n",
       " array([[-0.3113039 ,  0.17023903, -0.344167  ,  0.29333562, -0.02136618,\n",
       "         -0.42640874,  0.07107395, -0.303716  ],\n",
       "        [ 0.4666558 , -0.09738466,  0.28561953, -0.5307702 ,  0.09173138,\n",
       "         -0.05101156,  0.36804244,  0.3019262 ],\n",
       "        [ 0.41733384,  0.49172986,  0.20637679, -0.04217017, -0.39245188,\n",
       "         -0.19149938,  0.0083636 , -0.3909765 ],\n",
       "        [-0.34030655, -0.4450272 ,  0.39405745,  0.24152023,  0.16998816,\n",
       "         -0.42297208,  0.13509683, -0.14254525],\n",
       "        [ 0.01523143,  0.13871175,  0.05670753, -0.40592638,  0.16226704,\n",
       "         -0.09528312, -0.5008731 , -0.4883293 ],\n",
       "        [ 0.30663335, -0.03179562,  0.10163467, -0.27459598, -0.545521  ,\n",
       "         -0.45532885, -0.6964867 ,  0.49242485],\n",
       "        [ 0.5048044 , -0.01427656,  0.10933889,  0.19139433,  0.22286911,\n",
       "          0.24838883, -0.19801924,  0.11525041],\n",
       "        [ 0.43146405, -0.26046214,  0.363012  ,  0.01449148, -0.00475719,\n",
       "         -0.38059026, -0.33750382,  0.33250004],\n",
       "        [-0.25415793, -0.23779818,  0.34854904, -0.28544727,  0.4335922 ,\n",
       "         -0.12572679,  0.4125948 , -0.03685588],\n",
       "        [-0.3317091 ,  0.03989869,  0.14365958, -0.36386275,  0.382919  ,\n",
       "         -0.41627803, -0.4040991 , -0.08117577],\n",
       "        [-0.27333277,  0.09415042, -0.03617074,  0.44426197, -0.11926705,\n",
       "          0.36968815, -0.3668377 , -0.28041625],\n",
       "        [ 0.09219563, -0.52739406, -0.40611184,  0.44082528, -0.5018964 ,\n",
       "         -0.45023698,  0.18786281, -0.12455985]], dtype=float32)>,\n",
       " <tf.Variable 'dense_4/bias:0' shape=(8,) dtype=float32, numpy=\n",
       " array([-0.03775376,  0.        ,  0.2708472 , -0.22116502, -0.25540894,\n",
       "         0.        , -0.33988944,  0.        ], dtype=float32)>,\n",
       " <tf.Variable 'dense_5/kernel:0' shape=(8, 1) dtype=float32, numpy=\n",
       " array([[ 0.7070635 ],\n",
       "        [-0.31568348],\n",
       "        [-0.376311  ],\n",
       "        [ 0.3826491 ],\n",
       "        [ 0.70751894],\n",
       "        [ 0.5162066 ],\n",
       "        [ 0.31639606],\n",
       "        [-0.507975  ]], dtype=float32)>,\n",
       " <tf.Variable 'dense_5/bias:0' shape=(1,) dtype=float32, numpy=array([-0.26016304], dtype=float32)>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.metrics.base_metric.Mean at 0x26b41803730>,\n",
       " <keras.metrics.base_metric.MeanMetricWrapper at 0x26b4187b580>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'accuracy']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5637 - accuracy: 0.7048\n",
      "[0.5636965036392212, 0.7047756910324097]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# evaluate the keras model\n",
    "\n",
    "params = model.evaluate(X_train, y_train)\n",
    "print(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4682 - accuracy: 0.3444\n",
      "[0.4681567847728729, 0.3444283604621887]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# evaluate the keras model\n",
    "\n",
    "params2 = model2.evaluate(X_train, y_train)\n",
    "print(params2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The first is  the loss of the model on the dataset \n",
    "#the second will be the accuracy of the model on the dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 8ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.07676496],\n",
       "       [0.32679874],\n",
       "       [0.2980768 ],\n",
       "       [0.1958169 ],\n",
       "       [0.19340453],\n",
       "       [0.65564895],\n",
       "       [0.53160244],\n",
       "       [0.06477697],\n",
       "       [0.7209495 ],\n",
       "       [0.4430942 ],\n",
       "       [0.08983801],\n",
       "       [0.3696135 ],\n",
       "       [0.26639366],\n",
       "       [0.26468217],\n",
       "       [0.14098908],\n",
       "       [0.10388858],\n",
       "       [0.4801614 ],\n",
       "       [0.21119802],\n",
       "       [0.56241906],\n",
       "       [0.21169573],\n",
       "       [0.3938328 ],\n",
       "       [0.324295  ],\n",
       "       [0.6039736 ],\n",
       "       [0.88274485],\n",
       "       [0.2622409 ],\n",
       "       [0.40233302],\n",
       "       [0.42352116],\n",
       "       [0.15514813],\n",
       "       [0.69699883],\n",
       "       [0.55202734],\n",
       "       [0.11726232],\n",
       "       [0.89367163],\n",
       "       [0.12133009],\n",
       "       [0.14299046],\n",
       "       [0.21637274],\n",
       "       [0.33963153],\n",
       "       [0.8878929 ],\n",
       "       [0.22860038],\n",
       "       [0.2045186 ],\n",
       "       [0.73767805],\n",
       "       [0.57455546],\n",
       "       [0.8300781 ],\n",
       "       [0.38480002],\n",
       "       [0.30995128],\n",
       "       [0.19592984],\n",
       "       [0.65372926],\n",
       "       [0.08115727],\n",
       "       [0.15604576],\n",
       "       [0.798045  ],\n",
       "       [0.76104933],\n",
       "       [0.15905525],\n",
       "       [0.12832204],\n",
       "       [0.41906166],\n",
       "       [0.77305835],\n",
       "       [0.46175984],\n",
       "       [0.63794744],\n",
       "       [0.1165119 ],\n",
       "       [0.16881779],\n",
       "       [0.833671  ],\n",
       "       [0.22120301],\n",
       "       [0.28328452],\n",
       "       [0.3086376 ],\n",
       "       [0.48876172],\n",
       "       [0.7034928 ],\n",
       "       [0.3629342 ],\n",
       "       [0.46751773],\n",
       "       [0.5021851 ],\n",
       "       [0.06977903],\n",
       "       [0.23907553],\n",
       "       [0.06048495],\n",
       "       [0.24434333],\n",
       "       [0.49670637],\n",
       "       [0.5492419 ],\n",
       "       [0.601325  ],\n",
       "       [0.29002267],\n",
       "       [0.27718234],\n",
       "       [0.6034324 ]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "predictions = model.predict(X_test)\n",
    "predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "params = model.evaluate(X_test, y_test)\n",
    "print(params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ".152719170     ----0\n",
    ".116------------0\n",
    ".54-----------------1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# round predictions \n",
    "\n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "rounded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[40  7]\n",
      " [13 17]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test, rounded))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(confusion_matrix(y_test, rounded), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7402597402597403"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    53\n",
       "1    24\n",
       "dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(rounded).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'rand'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12156\\2705964138.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch' has no attribute 'rand'"
     ]
    }
   ],
   "source": [
    "\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'cuda'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12156\\3082617718.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch' has no attribute 'cuda'"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.cuda.is_available()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
